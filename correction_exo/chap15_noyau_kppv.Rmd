---
title: "Chapitre 15 : Estimateurs à noyau et $k$ plus proches voisins"
layout: default
output: 
  html_document:
    css: styles.css
    toc: true
    toc_float: true
    layout: default
header-includes:
  - \newcommand{\prob}{\mathbf P}
  - \newcommand{\esp}{\mathbf E}
  - \newcommand{\var}{\mathbf V}
  - \newcommand{\ind}{\mathbf 1}
---

## Exercice 1

  1. A
  2. B
  3. A
  
  
## Exercice 2

Il suffit de dériver la quantité à minimiser par rapport à $\beta_1$ et la valeur de $\beta_1$ qui annule cette dérivée :
$$-2\sum_{i=1}^n(y_i-\beta_1)p_i(x)=0\quad\Longleftrightarrow \quad\widehat\beta_1(x)=\frac{\sum_{i=1}^ny_ip_i(x)}{\sum_{i=1}^np_i(x)}.$$

## Exercice 3


## Exercice 4


1. En annulant la dérivée par rapport à $a$, on obtient
$$\widehat m(x)=\frac{\sum_{i=1}^ny_iK\left(\frac{\|x_i-x\|}{h}\right)}{\sum_{i=1}^nK\left(\frac{\|x_i-x\|}{h}\right)}.$$

2. On a
$$\var[\widehat m(x)]= \frac{\sigma^2}{\sum_{i=1}^nK\left(\frac{\|x_i-x\|}{h}\right)}$$
et
$$\esp[\widehat m(x)]-m(x)=\frac{\sum_{i=1}^n(m(x_i)-m(x))K\left(\frac{\|x_i-x\|}{h}\right)}{\sum_{i=1}^nK\left(\frac{\|x_i-x\|}{h}\right)}.$$

3. On a maintenant $|m(x_i)-m(x)|\leq L\|x_i-x\|$. Or 
$$K\left(\frac{\|x_i-x\|}{h}\right)$$
est non nul si et seulement si $\|x_i-x\|\leq h$. Donc pour tout $i=1,\dots,n$
$$L\|x_i-x\|K\left(\frac{\|x_i-x\|}{h}\right)\leq Lh K\left(\frac{\|x_i-x\|}{h}\right).$$
D'où le résultat.

4. On a
$$\var[\widehat m(x)]= \frac{\sigma^2}{\sum_{i=1}^nK\left(\frac{\|x_i-x\|}{h}\right)}=\frac{\sigma^2}{\sum_{i=1}^n\ind_{B_h}(x_i-x)}.$$
Or 
$$\sum_{i=1}^n\ind_{B_h}(x_i-x)\geq C_1n\textrm{Vol}(B_h)\geq C_1\gamma_dnh^d$$
où $\gamma_d=\pi^{d/2}/\Gamma(d/2+1)$. On a donc
$$\var[\widehat m(x)]\leq \frac{\sigma^2}{C_1\gamma_dnh^d}=\frac{C_2\sigma^2}{nh^d}$$
avec $C_2=1/(C_1\gamma_d)$.

5. On déduit
$$\esp[(\widehat m(x)-m(x))^2]\leq L^2h^2+\frac{C_2\sigma^2}{nh^d}.$$

6. Soit $M(h)$ le majorant ci-dessus. On a
$$M(h)'=2hL^2-\frac{C_2\sigma^2d}{n}h^{-d-1}.$$
La dérivée s'annule pour 
$$h_{opt}=\frac{2L^2}{C_2\sigma^2d}n^{-\frac{1}{d+2}}.$$
Lorsque $h=h_{opt}$ l'erreur quadratique vérifie
$$\esp[(\hat m(x)-m(x))^2]=\mathrm{O}\left(n^{-\frac{2}{d+2}}\right).$$
La vitesse diminue lorsque la dimension $d$ augmente, c'est le **fléau de la dimension**.

## Exercice 5

1. $\widehat\beta$ minimise $\sum_{i=1}^n(Y_i-\beta x_i)^2$. On a donc
$$\widehat\beta=\frac{\sum_{i=1}^nx_iY_i}{\sum_{i=1}^nx_i^2}.$$

2. On déduit
$$\esp[\widehat\beta]=\beta\quad\textrm{et}\quad\var(\widehat\beta)=\frac{\sigma^2}{\sum_{i=1}^nx_i^2}.$$

3. Comme 
$$\sum_{i=1}^nx_i^2=\frac{(n+1)(2n+1)}{6n},$$
on obtient le résultat demandé.




