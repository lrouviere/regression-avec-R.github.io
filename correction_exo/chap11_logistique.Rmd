---
title: "Chapitre 11 : régression logistique"
layout: default
output: 
  html_document:
    css: styles.css
    toc: true
    toc_float: true
    layout: default
header-includes:
  - \newcommand{\prob}{\mathbf P}
  - \newcommand{\esp}{\mathbf E}
  - \newcommand{\var}{\mathbf V}
  - \newcommand{\ind}{\mathbf 1}

#  html_document:
#    toc: true
#    toc_depth: 3
#  github_document:
#    toc: true
#    toc_depth: 3
---



## Exercice 1

  1. A
  2. A
  3. B
  4. A
  5. A
  6. A
  7. B
  8. A
  9. C
  10. D
  11. A, B
  12. C
  
  
## Exercice 2

1. 
```{r}
n <- 100
set.seed(48967365)
X <- sample(c("A","B","C"),100,replace=TRUE)
Y <- rep(0,n)
set.seed(487365)
Y[X=="A"] <- rbinom(sum(X=="A"),size=1,prob=0.95)
set.seed(4878365)
Y[X=="B"] <- rbinom(sum(X=="B"),size=1,prob=0.95)
set.seed(4653965)
Y[X=="C"] <- rbinom(sum(X=="C"),size=1,prob=0.05)
Y <- factor(Y)
donnees<-data.frame(Y,X)
```

2. 
```{r}
model1 <- glm(Y~X,data=donnees,family=binomial)
summary(model1)
```

On obtient les résultats du **test de Wald** sur la nullité des paramètres $\beta_0,\beta_2$ et $\beta_3$.

3.
```{r}
model2 <- glm(Y~C(X,base=3),data=donnees,family=binomial)
summary(model2)
```
On obtient les résultats du **test de Wald** sur la nullité des paramètres $\beta_0,\beta_1$ et $\beta_2$.

4. On remarque que dans **model1** on accepte la nullité de $\beta_2$ alors qu'on la rejette dans **model2**. Ceci est logique dans la mesure où ces tests dépendent de la contrainte identifiante choisie. Dans **model1** le test de nullité de $\beta_2$ permet de vérifier si $B$ à un effet similaire à $A$ sur $Y$. Dans **model2**, on compare l'effet de $B$ à celui de $C$. On peut donc conclure $A$ et $B$ ont des effets proches sur $Y$ alors que $B$ et $C$ ont un impact différent. Ceci est logique vu la façon dont les données ont été générées.

5. Tester l'effet global de $X$ sur $Y$ revient à tester si les coefficients $\beta_1,\beta_2$ et $\beta_3$ sont égaux, ce qui, compte tenu des contraintes revient à considérer les hypothèses nulles :
  * $\beta_2=\beta_3=0$ dans **model1** ;
  * $\beta_1=\beta_2=0$ dans **model2**.
  
On peut effectuer les tests de **Wald** ou du **rapport de vraisemblance**. On obtient les résultats du **rapport de vraisemblance** avec :

```{r message=FALSE, warning=FALSE}
library(car)
Anova(model1,type=3,test.statistic="LR")
Anova(model2,type=3,test.statistic="LR")
```


On remarque ici que ces deux tests sont identiques : ils ne dépendent pas de la contrainte identifiante choisie.

## Exercice 3

1.
```{r}
set.seed(1234)
X <- c(runif(50,-1,0),runif(50,0,1))
set.seed(5678)
Y <- c(rep(0,50),rep(1,50))
df <- data.frame(X,Y)
```


2.
```{r}
beta <- seq(0,100,by=0.01)
log_vrais <- function(X,Y,beta){
  LV <- rep(0,length(beta))
  for (i in 1:length(beta)){
    Pbeta <- exp(beta[i]*X)/(1+exp(beta[i]*X))
    LV[i] <- sum(Y*X*beta[i]-log(1+exp(X*beta[i])))
#    gradln[i] <- t(Xb)%*%(Yb-Pbeta)
  }
  return(LV)
}
LL <- log_vrais(X,Y,beta)
plot(beta,LL,type="l")
```

3.
```{r}
model <- glm(Y~X-1,data=df,family="binomial")
model$coef
```
On obtient un avertissement qui nous dit que l'algorithme d'optimisation n'a pas convergé.

4.

```{r}
Y1 <- Y;Y1[1] <- 1
LL1 <- log_vrais(X,Y1,beta)
plot(beta,LL1,type="l")
model1 <- glm(Y1~X-1,family="binomial")
model1$coef
```


## Exercice 4


## Exercice 5

On importe les données
```{r}
panne <- read.table("panne.txt",header=T)
head(panne)
```


1. La commande
```{r}
model <- glm(etat~.,data=panne,family=binomial)
model
```

ajuste le modèle

$$\log\left(\frac{p_\beta(x)}{1-p_\beta(x)}\right)=\beta_0+\beta_1x_1+\beta_2\mathsf{1}_{x_2=B}+\beta_3\mathsf{1}_{x_2=C}$$

où $x_1$ et $x_2$ désigne respectivement les variables **age** et **marque**. On obtient les estimateurs avec
```{r}
coef(model)
```

2. 

Il s'agit des tests de Wald pour tester l'effet des variables dans le modèle. Pour l'effet de marque, on va par exemple tester 
$$H_0:\beta_2=\beta_3=0\quad\text{contre}\quad H_1:\beta_2\neq 0\text{ ou }\beta_3\neq 0.$$
Sous $H_0$ la statistique de Wald suit une loi du $\chi^2$ à 4-2=2 degrés de liberté. Pour le test de la variable **age** le nombre de degrés de liberté manquant est 1. On retrouve cela dans la sortie

```{r}
library(car)
Anova(model,type=3,test.statistic="Wald")
```

3. Il s'agit cett fois du test du rapport de vraisemblance. Les degrés de liberté manquants sont identiques.

```{r}
Anova(model,type=3,test.statistic="LR")
```



4. 
  a). Le modèle s'écrit
  $$\log\left(\frac{p_\beta(x)}{1-p_\beta(x)}\right)=\beta_0+\beta_1\mathsf{1}_{x_2=A}+\beta_2\mathsf{1}_{x_2=B}.$$

  b). Le modèle ajusté ici est
    $$\log\left(\frac{p_\beta(x)}{1-p_\beta(x)}\right)=\gamma_0+\gamma_1\mathsf{1}_{x_2=B}+\gamma_2\mathsf{1}_{x_2=C}.$$

Par identification on a
$$\begin{cases}
\beta_0+\beta_1=\gamma_0 \\
\beta_0+\beta_2=\gamma_0+\gamma_1 \\
\beta_0=\gamma_0+\gamma_2 \\
\end{cases}
\Longleftrightarrow
\begin{cases}
\beta_0=\gamma_0+\gamma_2 \\
\beta_1=-\gamma_2 \\
\beta_2=\gamma_1-\gamma_2 \\
\end{cases}
\Longrightarrow
\begin{cases}
\widehat\beta_0=-0.92 \\
\widehat\beta_1=1.48 \\
\widehat\beta_2=1.05 \\
\end{cases}$$



On peut retrouver ces résultats avec

```{r}
glm(etat~C(marque,base=3),data=panne,family="binomial")
```


5. Il y a interaction si l'age agit différemment sur la panne en fonction de la marque.

6. Le modèle ajusté sur **R** est

$$\log\left(\frac{p_\beta(x)}{1-p_\beta(x)}\right)=\delta_0+\delta_1\mathsf{1}_{x_2=B}+\delta_2\mathsf{1}_{x_2=C}+\delta_3x_1+\delta_4x_1\mathsf{1}_{x_2=B}+\delta_5x_1\mathsf{1}_{x_2=C}.$$

On obtient ainsi par identification :
$$\begin{cases}
\alpha_0=\delta_0\\
\alpha_1=\delta_3\\
\beta_0=\delta_0+\delta_1\\
\beta_1=\delta_3+\delta_4\\
\gamma_0=\delta_0+\delta_2\\
\gamma_1=\delta_3+\delta_5
\end{cases}$$
On peut ainsi en déduire les valeurs des estimateurs que l'on peut retrouver avec la commande :

```{r}
glm(etat~-1+marque+marque:age,data=panne,family="binomial")
```


## Exercice 6

```{r}
df <- read.csv("logit_ex6.csv")
mod <- glm(Y~.,data=df,family=binomial)
mod1 <- glm(Y~X1,data=df,family=binomial)
summary(mod)
summary(mod1)
```


On remarque que la nullité du paramètre associé à **X1** est accepté dans le modèle avec uniquement **X1** alors qu'elle est refusée lorsqu'on considère **X1** et **X2** dans le modèle.

## Exercice 7

1. Le modèle s'écrit
$$log\left(\frac{p_\beta(x)}{1-p_\beta(x)}\right)=\beta_0+\beta_1x.$$

2. La log vraisemblance s'obtient avec
```{r}
p <- c(0.76,0.4,0.6,0.89,0.35)
Y <- c(1,0,0,1,1)
L1 <- log(prod(p^Y*(1-p)^(1-Y)))
L1
```

3. 
  
  a). On calcule les écart-type des estimateurs

```{r}
X1 <- c(0.47,-0.55,-0.01,1.07,-0.71)
X <- matrix(c(rep(1,5),X1),ncol=2)
W <- diag(p*(1-p))
SIG <- solve(t(X)%*%W%*%X)
sig <- sqrt(diag(SIG))
sig
```

On en déduit les statistiques de test :

```{r}
beta <- c(0.4383,1.5063)
beta/sig
```

  b). On peut faire le test de Wald et du rapport de vraisemblance. 
  
  c). La statistique de test vaut 0.8632411, on obtient donc la probabilité critique

```{r}
2*(1-pnorm(0.8632411))
```

On peut également effectuer un test du rapport de vraisemblance. Le modèle null sans **X1** a pour log-vraisemblance

```{r}
p0 <- 3/5
L0 <- log(prod(p0^Y*(1-p0)^(1-Y)))
L0
```
La statistique de test vaut donc

```{r}
2*(L1-L0)
```

et la probabilité critique vaut

```{r}
1-pchisq(2*(L1-L0),df=1)
```

On peut retrouver (aux arrondis près) les résultats de l'exercice avec

```{r}
X <- c(0.47,-0.55,-0.01,1.07,-0.71)
Y <- c(1,0,0,1,1)
df <- data.frame(X,Y)
model <- glm(Y~X,data=df,family="binomial")
logLik(model)
Anova(model,type=3,test.statistic = "Wald")
Anova(model,type=3,test.statistic = "LR")
```



## Exercice 8


## Exercice 9

1.  
    a). Par définition $g^\star(x)=1$ si $\prob(Y=1|X=x)\geq 0.5$, $0$ sinon. L'erreur de Bayes vaut $L^\star=L(g^\star)=\prob(g^\star(X)\neq Y)$.
    
    b). On a
  \begin{align*}
\prob(g(X)\neq Y|X=x) & = 1-\left(\prob(g(X)=Y,g(X)=1|X=x)+\prob(g(X)=Y,g(X)=0|X=x)\right) \\
& = 1-\left(\ind_{g(x)=1}\prob(Y=1|X=x)+\ind_{g(x)=0}\prob(Y=0|X=x)\right) \\
& = 1-(\ind_{g(x)=1}\eta(x)+\ind_{g(x)=0}(1-\eta(x))).
  \end{align*}
  
    c). On déduit
\begin{align*}
\prob(g(X)\neq Y|X=x)- & \prob(g^\star(X)\neq Y|X=x) \\
& = \eta(x)\left(\ind_{g^\star(x)=1}-\ind_{g(x)=1}\right)+(1-\eta(x)) \left(\ind_{g^\star(x)=0}-\ind_{g(x)=0}\right) \\
& = (2\eta(x)-1) \left(\ind_{g^\star(x)=1}-\ind_{g(x)=1}\right) \\
& \geq 0
\end{align*}
puisque par définition $g^\star=1$ si et seulement si $\eta(x)\geq 1/2$.

    d). En intégrant l'inégalité précédente par rapport à la loi de $X$ on conclut 
$$\prob(g(X)\neq Y)\geq \prob(g^\star(X)\neq Y).$$

2. Si $x\leq 0$ alors $\prob(Y=1|X=x)=\prob(U\leq 2)=1/5$ donc $g^\star(x)=0$. On montre de même que $g^\star(x)=1$ si $x>0$. Il reste à calculer l'erreur de Bayes :
$$L^\star=\prob(g^\star(X)\neq Y|X\leq 0)\prob(X\leq 0)+\prob(g^\star(X)\neq Y|X>0)\prob(X>0).$$
Or
$$\prob(g^\star(X)\neq Y|X\leq 0)=\prob(Y\neq 0|X\leq 0)=\prob(U\leq 2|X\leq 0)=\frac{1}{5}$$
et
$$\prob(g^\star(X)\neq Y|X>0)=\prob(Y\neq 1|X>0)=\prob(U<1|X\leq 0)=\frac{1}{10}.$$
On obtient 
$$L^\star=\frac{1}{5}\,\frac{1}{2}+\frac{1}{10}\,\frac{1}{2}=\frac{3}{20}.$$


## Exercice 10

2.
  a).
```{r}
df <- read.csv("logit_ex6.csv")
set.seed(1234)
perm <- sample(nrow(df))
dapp <- df[perm[1:300],]
dval <- df[-perm[1:300],]
```

  b). 
  
```{r}
mod <- glm(Y~.,data=dapp,family="binomial")
```

  c).
  
```{r}
score <- predict(mod,newdata = dval)
```

  d).
  
```{r}
D0 <- which(dval$Y==0)
D1 <- which(dval$Y==1)
S0 <-score[D0]
S1 <- score[D1]
S01 <- expand.grid(S0=S0,S1=S1)
mean(S01$S1>S01$S0)
```
  
  e).
  
```{r message=FALSE, warning=FALSE}
library(pROC) 
auc(dval$Y,score)
```
  
  
  
## Exercice 11
